{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1 - Installing the required dependencies \n",
        "Before we can begin we need to make sure we have all the required dependencies installed in our notebook kernel. You will also want to ensure that you have the configured the correct runtime in the notebook (e.g. GPU or CPU)"
      ],
      "metadata": {
        "id": "Vj5le65hDNRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In order to avoid future dependency issues we have frozen the versions. \n",
        "# This means you may have to alter these as time goes by and new releases\n",
        "# are available. \n",
        "!pip install transformers==4.25.1\n",
        "!pip install datasets==2.8.0\n",
        "!pip install evaluate==0.4.0\n",
        "!pip install accelerate==0.15.0"
      ],
      "metadata": {
        "id": "9ws1jlBY3O6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2 - Persisting models and accessing training data\n",
        "We need a way to persist our models and tokenizers along with an easy way to pull the training set without having to deal with uploading/downloading to a new runtime. This will save a lot of headache and give us the ability to infer from the model in a separate notebook seamlessly. "
      ],
      "metadata": {
        "id": "8eQQie5pEUd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "\n",
        "# Create two new working directories if they do not already exist\n",
        "import os\n",
        "from os import path\n",
        "\n",
        "new_paths = ['/content/drive/MyDrive/models','/content/drive/MyDrive/training_data']\n",
        "for p in new_paths:\n",
        "  if path.exists(p) == False:\n",
        "    os.mkdir(p)\n",
        "\n",
        "# IMPORTANT: At this point you will need to upload a text file containing your training data \n",
        "# to the /content/drive/MyDrive/training_data directory with the name training-set.txt.\n",
        "# You only have to do this once unless you want to use new training data. "
      ],
      "metadata": {
        "id": "A22oEQDqE1sF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3 - The Setup - Loading our model and tokenizer\n",
        "Here we will fetch our base foundation model and its associated tokenizer. Depending on which LLM you choose the Auto feature will determine the optimal downloads and load them into the appropriate variables so that we can fine-tune and retrain in next steps. It may take a while to download very large models so have a cup of coffee in the meantime. "
      ],
      "metadata": {
        "id": "V1v_SoNNIHp2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRyNTGHjrjDD"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments, AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling, AutoModelForCausalLM\n",
        "import time\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import sklearn\n",
        "\n",
        "# Lets create some inline metrics for future reference\n",
        "start = time.time()\n",
        "\n",
        "print(\"Loading model\")\n",
        "# We recommend one of the following: EleutherAI/gpt-neo-125M, EleutherAI/gpt-j-6B, EleutherAI/gpt-neo-1.3B, EleutherAI/gpt-neo-2.7B\n",
        "# NOTE: For any model greater than 125M parameters you are going to need Premium GPU \n",
        "model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", \n",
        "                                             device_map=\"auto\")\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "print(\"Loading tokenizer\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, torch_dtype=\"auto\", \n",
        "                                          device_map=\"auto\")\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4 - Preparing our Training Set\n",
        "Now that we have our model and tokenizer we can use the huggingface dataset library and tokenizer to prepare our train and test sets to train the base foundation model. "
      ],
      "metadata": {
        "id": "Dz5kS9goKHB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "print(\"Loading dataset\")\n",
        "\n",
        "training_set = \"/content/drive/MyDrive/training_data/training-set.txt\"\n",
        "\n",
        "# TODO: Add a test set for eval\n",
        "# Here we want to load the dataset and sample by paragraph \n",
        "current_dataset = load_dataset(\"text\", data_files={\"train\": training_set, \n",
        "                                                   \"test\": training_set}, \n",
        "                               sample_by=\"paragraph\")\n",
        "\n",
        "current_dataset['train'] = current_dataset['train']\n",
        "\n",
        "# Once we have extracted text by paragraph we need this function to convert it \n",
        "# into the tokens that are expected by the model. \n",
        "def tokenize_function(examples):\n",
        "    current_tokenizer_result = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "    return current_tokenizer_result\n",
        "\n",
        "\n",
        "print(\"Tokenizing dataset\")\n",
        "tokenized_datasets = current_dataset.map(tokenize_function, batched=True)\n",
        "small_train_dataset = tokenized_datasets[\"train\"] #.select(range(75))\n",
        "small_eval_dataset = small_train_dataset\n"
      ],
      "metadata": {
        "id": "GjjBi4md59Lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5 - Model Training\n",
        "Excellent we have everything we need as input for training the model in an unsupervised fashion. Now lets begin the training. Keep in mind this may take some time depending on your hardware setup and the chosen model. "
      ],
      "metadata": {
        "id": "oZxzTmi4M3Rl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "import sklearn\n",
        "import torch, gc\n",
        "\n",
        "# I have added this here in order to free as much memory as possible right before\n",
        "# we go into training as it is quite memory intense. \n",
        "gc.collect()\n",
        "# Lets test for cuda if you are using GPU\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.empty_cache()\n",
        "  print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
        "\n",
        "print(\"Preparing training arguments\")\n",
        "# If you are running on CPU you can change no_cuda to True\n",
        "training_args = TrainingArguments(output_dir=new_paths[0],\n",
        "                                  report_to='all',\n",
        "                                  logging_dir='./logs',\n",
        "                                  per_device_train_batch_size=1,\n",
        "                                  label_names=['input_ids', 'attention_mask'],  # 'logits', 'past_key_values'\n",
        "                                  num_train_epochs=1,\n",
        "                                  no_cuda=False,\n",
        "                                  )\n",
        "\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"Starting training\")\n",
        "trainer.train()\n",
        "print(f\"Finished fine-tuning in {time.time() - start}\")"
      ],
      "metadata": {
        "id": "c3rG47YE6Buu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Don't forget to save our tokenizer, model checkpoints for inference! \n",
        "trainer.save_model()\n",
        "tokenizer.save_pretrained(new_paths[0])"
      ],
      "metadata": {
        "id": "vcNsN4yY3pRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next Steps - Inference \n",
        "Great job! You've trained your model now lets get to the fun part inference. Head on over to the colab_expert_system_inference.ipynb to test out your model. "
      ],
      "metadata": {
        "id": "FtsEAsb7RPFV"
      }
    }
  ]
}