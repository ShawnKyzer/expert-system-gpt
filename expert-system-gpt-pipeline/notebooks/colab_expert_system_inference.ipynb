{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1 - Installing the required dependencies\n",
        "Now that we have successfully trained our model we want to generate text or infer from some given context such as a question, sentence or command. Before we can begin lets install the minimum required dependencies for this task. \n"
      ],
      "metadata": {
        "id": "HLYcfhRLWLOI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5hIxa2-DQMf"
      },
      "outputs": [],
      "source": [
        "# In order to avoid future dependency issues we have frozen the versions. \n",
        "# This means you may have to alter these as time goes by and new releases\n",
        "# are available. \n",
        "!pip install transformers==4.25.1\n",
        "!pip install accelerate==0.15.0\n",
        "!pip install gradio==3.16.2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2 - Accessing the model checkpoint\n",
        "In our previous lesson we trained a model and saved to our google drive. Now we just need to mount the drive and set the location where we saved it for future steps. "
      ],
      "metadata": {
        "id": "0_SyVQ7BW9eD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "checkpoint = '/content/drive/MyDrive/models'\n",
        "\n",
        "import os\n",
        "from os import path\n",
        "\n",
        "if path.exists(checkpoint) == False:\n",
        "    print(\"Unable to find the model directory are you sure the path is correct\")\n"
      ],
      "metadata": {
        "id": "7aPkKG2DXVnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3 - Loading the Model Checkpoint and Tokenizer\n",
        "This part is pretty easy thanks to the hard work of the folks at huggingface. We just need to load these two into memory so that we can encode/decode inputs/outputs for our model and display the text generated from the model in human readable form. "
      ],
      "metadata": {
        "id": "E_417-PlYP-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch, gc\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=\"auto\", \n",
        "                                             device_map=\"auto\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint, local_files_only=True)"
      ],
      "metadata": {
        "id": "F5joM9L_81os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4 - Creating a UI and Testing with Prompt Engineering\n",
        "\n",
        "Now that we have our model loaded in memory we want to run some inferences. In this section we will go into detail on some prompt engineering. However, lets first put it in a nice gradio ui so we can run any number of inferences. We will also explore the various hyperparameters and how to tune them. "
      ],
      "metadata": {
        "id": "KEhrs1RpY58O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "title = \"expert-system-gpt\"\n",
        "\n",
        "examples = [\n",
        "    [\"{Insert your custom prompts here}\"],\n",
        "    [\"The Moon's orbit around Earth has\"],\n",
        "    [\"The smooth Borealis basin in the Northern Hemisphere covers 40%\"],\n",
        "]\n",
        "\n",
        "def generate_response(text: str):\n",
        "    inputs = tokenizer.encode(text, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(inputs,\n",
        "                             early_stopping=True,\n",
        "                             max_new_tokens=250, \n",
        "                             temperature=0.0, \n",
        "                             repetition_penalty = 5.0,\n",
        "                             top_k=5, \n",
        "                             top_p=0.95)\n",
        "\n",
        "    return tokenizer.decode(outputs[0])\n",
        "\n",
        "demo = gr.Interface(fn=generate_response,\n",
        "    inputs=gr.Textbox(lines=5, max_lines=6, label=\"Input Text\"),\n",
        "    title=title,\n",
        "    outputs=\"text\",\n",
        "    examples=examples,\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()\n"
      ],
      "metadata": {
        "id": "UGxMEnxCUYhd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}